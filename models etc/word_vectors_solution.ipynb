{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, delimiter='\\t', encoding='utf-8', header=None, names=['tag', 'sentence'], error_bad_lines=False, warn_bad_lines=True)\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "        return data\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error loading dataset.\")\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    \n",
    "    words = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def prepare_data_tfidf(data_sample):\n",
    "    data_sample['preprocessed_text'] = data_sample['sentence'].apply(preprocess_text)\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(data_sample['preprocessed_text']).toarray()\n",
    "    y = data_sample['tag']\n",
    "    return X, y\n",
    "\n",
    "def tune_hyperparameters(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'Recall: {recall}')\n",
    "        print(f'F1-Score: {f1}')\n",
    "        return accuracy, precision, recall, f1\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model {model.__class__.__name__}: {e}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "def main():\n",
    "    # Load data\n",
    "    data = load_data(r'C:\\Users\\adria\\Documents\\GitHub\\project-nlp\\TRAINING_DATA.txt')\n",
    "    if data is None:\n",
    "        return\n",
    "\n",
    "    # Sample data for quick testing\n",
    "    data_sample = data.sample(n=2000, random_state=42)\n",
    "\n",
    "    # Prepare data with TF-IDF\n",
    "    X, y = prepare_data_tfidf(data_sample)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Gaussian Naive Bayes\n",
    "    print(\"\\nGaussian Naive Bayes Classifier\")\n",
    "    gaussian_nb = GaussianNB()\n",
    "    evaluate_model(gaussian_nb, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Hyperparameter tuning for Gaussian Naive Bayes\n",
    "    print(\"\\nTuning Gaussian Naive Bayes\")\n",
    "    param_grid_gnb = {\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "    }\n",
    "    tuned_gnb = tune_hyperparameters(gaussian_nb, param_grid_gnb, X_train, y_train)\n",
    "    print(\"Tuned Gaussian Naive Bayes\")\n",
    "    evaluate_model(tuned_gnb, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Hyperparameter tuning for Logistic Regression\n",
    "    param_grid_lr = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    }\n",
    "    tuned_lr = tune_hyperparameters(LogisticRegression(), param_grid_lr, X_train, y_train)\n",
    "    \n",
    "    # Hyperparameter tuning for Random Forest\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    tuned_rf = tune_hyperparameters(RandomForestClassifier(), param_grid_rf, X_train, y_train)\n",
    "\n",
    "    # Hyperparameter tuning for Gradient Boosting\n",
    "    param_grid_gb = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    tuned_gb = tune_hyperparameters(GradientBoostingClassifier(), param_grid_gb, X_train, y_train)\n",
    "\n",
    "    # Hyperparameter tuning for Support Vector Machine\n",
    "    param_grid_svc = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "    tuned_svc = tune_hyperparameters(make_pipeline(StandardScaler(), SVC()), param_grid_svc, X_train, y_train)\n",
    "\n",
    "    # Hyperparameter tuning for XGBoost\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    tuned_xgb = tune_hyperparameters(XGBClassifier(), param_grid_xgb, X_train, y_train)\n",
    "\n",
    "    # Hyperparameter tuning for Decision Tree\n",
    "    param_grid_dt = {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': [None, 'auto', 'sqrt', 'log2']\n",
    "    }\n",
    "    tuned_dt = tune_hyperparameters(DecisionTreeClassifier(), param_grid_dt, X_train, y_train)\n",
    "\n",
    "    # Hyperparameter tuning for Multi-layer Perceptron\n",
    "    param_grid_mlp = {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "        'mlpclassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpclassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpclassifier__alpha': [0.0001, 0.001, 0.01],\n",
    "        'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "    }\n",
    "    tuned_mlp = tune_hyperparameters(make_pipeline(StandardScaler(), MLPClassifier()), param_grid_mlp, X_train, y_train)\n",
    "\n",
    "    # Evaluate tuned models\n",
    "    tuned_models = {\n",
    "        \"Logistic Regression\": tuned_lr,\n",
    "        \"Random Forest\": tuned_rf,\n",
    "        \"Gradient Boosting\": tuned_gb,\n",
    "        \"Support Vector Machine\": tuned_svc,\n",
    "        \"XGBoost\": tuned_xgb,\n",
    "        \"Decision Tree\": tuned_dt,\n",
    "        \"Multi-layer Perceptron\": tuned_mlp\n",
    "    }\n",
    "\n",
    "    for name, model in tuned_models.items():\n",
    "        print(f\"\\n{name} (Tuned)\")\n",
    "        evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "  # Ensemble: Voting Classifier with tuned models\n",
    "    print(\"\\nEnsemble: Voting Classifier (Tuned)\")\n",
    "    voting_classifier = VotingClassifier(estimators=[(name, model) for name, model in tuned_models.items()])\n",
    "    evaluate_model(voting_classifier, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Ensemble: Stacking Classifier with tuned models\n",
    "    print(\"\\nEnsemble: Stacking Classifier (Tuned)\")\n",
    "    stacking_classifier = StackingClassifier(estimators=[(name, model) for name, model in tuned_models.items()], final_estimator=LogisticRegression())\n",
    "    evaluate_model(stacking_classifier, X_train, X_test, y_train, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import gensim.downloader as api\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(r'C:\\Users\\adria\\Documents\\GitHub\\project-nlp\\TRAINING_DATA.txt', delimiter='\\t', encoding='utf-8', header=None, names=['tag', 'sentence'], error_bad_lines=False, warn_bad_lines=True)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error loading dataset.\")\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('spanish'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words\n",
    "\n",
    "# Preprocess text data\n",
    "data['preprocessed_text'] = data['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Load pre-trained FastText model\n",
    "fasttext_vectors = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "# Sample data\n",
    "data_sample = data.sample(n=2000, random_state=42)\n",
    "\n",
    "# Function to calculate average word vector for a sentence\n",
    "def average_word_vector(words, model):\n",
    "    vectors = [model[word] for word in words if word in model.key_to_index]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Calculate average word vectors for each sentence in the sample\n",
    "X = np.array([average_word_vector(words, fasttext_vectors) for words in data_sample['preprocessed_text']])\n",
    "y = data_sample['tag']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to tune hyperparameters\n",
    "def tune_hyperparameters(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Model Training and Evaluation function\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1-Score: {f1}')\n",
    "\n",
    "# Train and evaluate Logistic Regression\n",
    "print(\"Logistic Regression\")\n",
    "log_reg = LogisticRegression()\n",
    "param_grid_lr = {'C': [1.0], 'solver': ['newton-cg']}\n",
    "tuned_lr = tune_hyperparameters(log_reg, param_grid_lr, X_train, y_train)\n",
    "evaluate_model(tuned_lr, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Train and evaluate Gradient Boosting\n",
    "print(\"\\nGradient Boosting\")\n",
    "gradient_boosting = GradientBoostingClassifier()\n",
    "evaluate_model(gradient_boosting, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Train and evaluate Support Vector Machine\n",
    "print(\"\\nSupport Vector Machine\")\n",
    "svc = SVC()\n",
    "evaluate_model(svc, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Train and evaluate Gaussian Naive Bayes\n",
    "print(\"\\nGaussian Naive Bayes Classifier\")\n",
    "gaussian_nb = GaussianNB()\n",
    "param_grid_gnb = {'var_smoothing': [1e-7]}\n",
    "tuned_gnb = tune_hyperparameters(gaussian_nb, param_grid_gnb, X_train, y_train)\n",
    "evaluate_model(tuned_gnb, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import gensim.downloader as api\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    data = pd.read_csv(r'C:\\Users\\adria\\Documents\\GitHub\\project-nlp\\TRAINING_DATA.txt', delimiter='\\t', encoding='utf-8', header=None, names=['tag', 'sentence'], error_bad_lines=False, warn_bad_lines=True)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error loading dataset.\")\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('spanish'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Preprocess text data\n",
    "data['preprocessed_text'] = data['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Sample data\n",
    "data_sample = data.sample(n=2000, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_sample['preprocessed_text'], data_sample['tag'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained FastText vectors\n",
    "model = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "\n",
    "# Function to get average vector representation of a sentence using FastText embeddings\n",
    "def get_average_vector(text, model):\n",
    "    words = text.split()\n",
    "    vectors = [model[word] for word in words if word in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Convert text data into FastText embeddings\n",
    "X_train_embeddings = np.array([get_average_vector(text, model) for text in X_train])\n",
    "X_test_embeddings = np.array([get_average_vector(text, model) for text in X_test])\n",
    "\n",
    "# Train logistic regression classifier\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train_embeddings, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "y_pred = classifier.predict(X_test_embeddings)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Evaluation Metrics for Logistic Regression Classifier with FastText Embeddings:\")\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    data = pd.read_csv(r'C:\\Users\\adria\\Documents\\GitHub\\project-nlp\\TRAINING_DATA.txt', delimiter='\\t', encoding='utf-8', header=None, names=['tag', 'sentence'], error_bad_lines=False, warn_bad_lines=True)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error loading dataset.\")\n",
    "\n",
    "# Sample data\n",
    "data_sample = data.sample(n=2000, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = data_sample['sentence']\n",
    "y = data_sample['tag']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load XLM-R model and tokenizer\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to preprocess text using XLM-R tokenizer\n",
    "def preprocess_text(text_list):\n",
    "    return [tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")['input_ids'] for text in text_list]\n",
    "\n",
    "# Custom transformer to extract XLM-R embeddings\n",
    "class XlmREmbeddingExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**X[0])  # Access the input dictionary\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).tolist()  # Convert tensor to list\n",
    "        return embeddings\n",
    "\n",
    "# Pipeline for feature extraction and model training with Gaussian Naive Bayes\n",
    "pipeline_nb = Pipeline([\n",
    "    ('preprocessor', FunctionTransformer(preprocess_text)),\n",
    "    ('embedding_extractor', XlmREmbeddingExtractor()),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Fit pipeline with Gaussian Naive Bayes\n",
    "pipeline_nb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation on test data with Gaussian Naive Bayes\n",
    "y_pred_nb = pipeline_nb.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "precision_nb = precision_score(y_test, y_pred_nb, average='weighted')\n",
    "recall_nb = recall_score(y_test, y_pred_nb, average='weighted')\n",
    "f1_nb = f1_score(y_test, y_pred_nb, average='weighted')\n",
    "\n",
    "print(\"Pipeline Classifier with Gaussian Naive Bayes:\")\n",
    "print(f'Accuracy: {accuracy_nb}')\n",
    "print(f'Precision: {precision_nb}')\n",
    "print(f'Recall: {recall_nb}')\n",
    "print(f'F1-Score: {f1_nb}')\n",
    "\n",
    "# Pipeline for feature extraction and model training with Logistic Regression\n",
    "pipeline_lr = Pipeline([\n",
    "    ('preprocessor', FunctionTransformer(preprocess_text)),\n",
    "    ('embedding_extractor', XlmREmbeddingExtractor()),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit pipeline with Logistic Regression\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation on test data with Logistic Regression\n",
    "y_pred_lr = pipeline_lr.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "precision_lr = precision_score(y_test, y_pred_lr, average='weighted')\n",
    "recall_lr = recall_score(y_test, y_pred_lr, average='weighted')\n",
    "f1_lr = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "print(\"\\nPipeline Classifier with Logistic Regression:\")\n",
    "print(f'Accuracy: {accuracy_lr}')\n",
    "print(f'Precision: {precision_lr}')\n",
    "print(f'Recall: {recall_lr}')\n",
    "print(f'F1-Score: {f1_lr}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
