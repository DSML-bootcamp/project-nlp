{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step by step for the Project NLP**\n",
    "\n",
    "**1. Data Preprocessing -** *cleaning and preparing data*:\n",
    "- Remove all the special characters\n",
    "- Remove numbers\n",
    "- Remove all single characters\n",
    "- Remove single characters from the start\n",
    "- Substitute multiple spaces with single space\n",
    "- Convert to Lowercase\n",
    "- Remove stopwords\n",
    "- Lemmatization\n",
    "\n",
    "**2. Feature extraction -** *Transforming the text data into numerical features*\n",
    "- Using Sentence-Transformers extraction technique\n",
    "\n",
    "**3. Model Selection -** *Choosing an appropriate machine learning model*\n",
    "- Logistic Regression Model (good to predict the probability of an event occurring)\n",
    "\n",
    "**4. Model Training -** *Training the models on the prepared data*\n",
    "\n",
    "**5. Model Evaluation -** *Assessing the performance of the models*\n",
    "\n",
    "**6. Prediction -** *Preparing the model to classify unseen data*\n",
    "\n",
    "**7. Predict in a new given dataset** *Tagging new and unlabeled data*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanas\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aanas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aanas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install sentence-transformers\n",
    "\n",
    "import pandas as pd\n",
    "import nltk \n",
    "import re \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#Needed packages:\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cuando conocí a Janice en 2013 , una familia n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Hwang habló en Sur de este año por Southwest M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Usted podría pensar Katy Perry y Robert Pattin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Cualquiera que haya volado los cielos del crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bueno , este cantante tendrá un LARGO tiempo p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           sentence\n",
       "0      1  Cuando conocí a Janice en 2013 , una familia n...\n",
       "1      0  Hwang habló en Sur de este año por Southwest M...\n",
       "2      1  Usted podría pensar Katy Perry y Robert Pattin...\n",
       "3      1  Cualquiera que haya volado los cielos del crea...\n",
       "4      1  Bueno , este cantante tendrá un LARGO tiempo p..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read training data and test data\n",
    "\n",
    "training_data = pd.read_csv(\"TRAINING_DATA.txt\", sep='\\t', header=None, names=['label','sentence'])\n",
    "\n",
    "display(training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label       False\n",
       "sentence    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quick check for missing values\n",
    "training_data.isna().any()\n",
    "\n",
    "#No NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for preprocessing\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(['spanish']))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove all special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^[a-zA-Z]\\s', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    #Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    #Lemmatization\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cuando conocí a Janice en 2013 , una familia n...</td>\n",
       "      <td>conoc janice familia necesitaba puntos promedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Hwang habló en Sur de este año por Southwest M...</td>\n",
       "      <td>hwang habl sur ao southwest music and medium c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Usted podría pensar Katy Perry y Robert Pattin...</td>\n",
       "      <td>usted podra pensar katy perry robert pattinson...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Cualquiera que haya volado los cielos del crea...</td>\n",
       "      <td>cualquiera volado cielos creador escuchado act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bueno , este cantante tendrá un LARGO tiempo p...</td>\n",
       "      <td>bueno cantante tendr largo tiempo sentir an m ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           sentence  \\\n",
       "0      1  Cuando conocí a Janice en 2013 , una familia n...   \n",
       "1      0  Hwang habló en Sur de este año por Southwest M...   \n",
       "2      1  Usted podría pensar Katy Perry y Robert Pattin...   \n",
       "3      1  Cualquiera que haya volado los cielos del crea...   \n",
       "4      1  Bueno , este cantante tendrá un LARGO tiempo p...   \n",
       "\n",
       "                                    cleaned_sentence  \n",
       "0  conoc janice familia necesitaba puntos promedi...  \n",
       "1  hwang habl sur ao southwest music and medium c...  \n",
       "2  usted podra pensar katy perry robert pattinson...  \n",
       "3  cualquiera volado cielos creador escuchado act...  \n",
       "4  bueno cantante tendr largo tiempo sentir an m ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying the preprocessing fucntion to the training data\n",
    "\n",
    "training_data['cleaned_sentence'] = training_data['sentence'].apply(preprocess_text)\n",
    "\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into training and test sets\n",
    "x = training_data['sentence']\n",
    "y = training_data['label']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanas\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Using sentencetransformer for embeddings\n",
    "sentence_model = SentenceTransformer('hiiamsid/sentence_similarity_spanish_es')\n",
    "\n",
    "#transforming the training and test data\n",
    "x_train_embeddings = sentence_model.encode(x_train.tolist())\n",
    "x_test_embeddings = sentence_model.encode(x_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing and training the logistic regression\n",
    "model = LogisticRegression(max_iter=1000, random_state=42) \n",
    "\n",
    "#model.fit(x_train_tfidf, y_train)\n",
    "model.fit(x_train_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "Precision: 0.6725024576721899\n",
      "Recall: 0.6716917922948074\n",
      "F1-Score: 0.6714577673745435\n",
      "Accuracy: 0.6716917922948074\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model on test data\n",
    "#y_pred = model.predict(x_test_tfidf)\n",
    "y_pred = model.predict(x_test_embeddings)\n",
    "\n",
    "#printing classification report\n",
    "report=classification_report(y_test, y_pred, output_dict=True)\n",
    "print(\"Classification Report:\")\n",
    "print(f\"Precision: {report['weighted avg']['precision']}\")\n",
    "print(f\"Recall: {report['weighted avg']['recall']}\")\n",
    "print(f\"F1-Score: {report['weighted avg']['f1-score']}\")\n",
    "\n",
    "#accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to REAL_DATA_PREDICTED.csv\n"
     ]
    }
   ],
   "source": [
    "# Reading the real data\n",
    "real_data = pd.read_csv(\"REAL_DATA.txt\", sep='\\t', header=None, names=['label', 'sentence'], on_bad_lines='skip', engine='python')\n",
    "\n",
    "# Applying the preprocessing function to the real data\n",
    "real_data['cleaned_sentence'] = real_data['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Transforming the real data using the SentenceTransformer\n",
    "real_data_embeddings = sentence_model.encode(real_data['cleaned_sentence'])\n",
    "\n",
    "# Predicting the tags for the real data\n",
    "real_data['predicted_label'] = model.predict(real_data_embeddings)\n",
    "\n",
    "# Saving the results to a new CSV file\n",
    "real_data[['predicted_label','sentence']].to_csv('REAL_DATA_PREDICTED.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to REAL_DATA_PREDICTED.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
