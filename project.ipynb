{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the df and load the data inside with two columns\n",
    "column_names = ['label', 'text']\n",
    "data = pd.read_fwf('TRAINING_DATA.txt', header=None, names=column_names)\n",
    "\n",
    "# 17877 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cuando conocí a Janice en 2013 , una familia n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Hwang habló en Sur de este año por Southwest M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Usted podría pensar Katy Perry y Robert Pattin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Cualquiera que haya volado los cielos del crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bueno , este cantante tendrá un LARGO tiempo p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Cuando conocí a Janice en 2013 , una familia n...\n",
       "1      0  Hwang habló en Sur de este año por Southwest M...\n",
       "2      1  Usted podría pensar Katy Perry y Robert Pattin...\n",
       "3      1  Cualquiera que haya volado los cielos del crea...\n",
       "4      1  Bueno , este cantante tendrá un LARGO tiempo p..."
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see it\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a train_test_split for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only a certain amount of data to make training and predicting faster to tune parameters\n",
    "X_train = X_train[:1500]\n",
    "X_test = X_test[:1000]\n",
    "y_train = y_train[:1500]\n",
    "y_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise the tokenizer for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_for_bert(text):\n",
    "    # Clean the text: join back into a single string without modifying the content\n",
    "    text = ' '.join(text.split())\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Add [CLS] and [SEP] tokens\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the text\n",
    "train_encodings = [preprocess_text_for_bert(text) for text in X_train]\n",
    "test_encodings = [preprocess_text_for_bert(text) for text in X_test]\n",
    "\n",
    "# Convert tokenized text data into numerical IDs for train and test sets\n",
    "train_input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in train_encodings]\n",
    "test_input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in test_encodings]\n",
    "\n",
    "# Pad sequences to a fixed length for train and test sets\n",
    "max_length = max(max(len(ids) for ids in train_input_ids), max(len(ids) for ids in test_input_ids))\n",
    "train_input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in train_input_ids]\n",
    "test_input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in test_input_ids]\n",
    "\n",
    "# Convert input_ids lists into PyTorch tensors for train and test sets\n",
    "train_input_ids = torch.tensor(train_input_ids)\n",
    "test_input_ids = torch.tensor(test_input_ids)\n",
    "\n",
    "# Create attention masks to indicate which tokens are actual words and which are padding tokens for train and test sets\n",
    "train_attention_masks = torch.where(train_input_ids != tokenizer.pad_token_id, torch.tensor(1), torch.tensor(0))\n",
    "test_attention_masks = torch.where(test_input_ids != tokenizer.pad_token_id, torch.tensor(1), torch.tensor(0))\n",
    "\n",
    "# Create labels tensors for train and test sets\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "test_labels = torch.tensor(y_test.values)\n",
    "\n",
    "# Create TensorDatasets for train and test sets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the batch size and create dataloaders to feed the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders for train and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "class CustomBERTForClassification(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomBERTForClassification, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', num_labels=num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        return logits\n",
    "\n",
    "# Initialize the fine-tuning model\n",
    "num_labels = 2  # Number of output labels (machine-written, human-written)\n",
    "model = CustomBERTForClassification(num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function, optimiser, num of epochs and move model to gpu ( Nvidia 4060 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBERTForClassification(\n",
       "  (bert): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Define number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Move model and data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the training loop and print loss per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6350215077400208\n",
      "Epoch 2, Loss: 0.389578825774345\n",
      "Epoch 3, Loss: 0.1716717522036522\n",
      "Epoch 4, Loss: 0.08509116011493384\n",
      "Epoch 5, Loss: 0.05483895638323528\n",
      "Epoch 6, Loss: 0.03891185656724934\n",
      "Epoch 7, Loss: 0.03221550868933109\n",
      "Epoch 8, Loss: 0.034046128768711645\n",
      "Epoch 9, Loss: 0.039166630590037305\n",
      "Epoch 10, Loss: 0.07996122936837058\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_masks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the trained model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "# torch.save(model.state_dict(), \"test_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model based on accuracy, precision, recall and f1 score at different threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with 0.7 threshold\n",
      "\n",
      "Validation Accuracy: 0.67\n",
      "Validation Precision: 0.6348973607038123\n",
      "Validation Recall: 0.8424124513618677\n",
      "Validation F1-Score: 0.7240802675585284\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Performance with 0.6 threshold\n",
      "\n",
      "Validation Accuracy: 0.662\n",
      "Validation Precision: 0.6253561253561254\n",
      "Validation Recall: 0.8540856031128404\n",
      "Validation F1-Score: 0.7220394736842105\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Performance with 0.5 threshold\n",
      "\n",
      "Validation Accuracy: 0.652\n",
      "Validation Precision: 0.6130790190735694\n",
      "Validation Recall: 0.8754863813229572\n",
      "Validation F1-Score: 0.7211538461538461\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Performance with 0.3 threshold\n",
      "\n",
      "Validation Accuracy: 0.619\n",
      "Validation Precision: 0.5813953488372093\n",
      "Validation Recall: 0.9241245136186771\n",
      "Validation F1-Score: 0.7137490608564989\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Performance with 0.2 threshold\n",
      "\n",
      "Validation Accuracy: 0.593\n",
      "Validation Precision: 0.5608646188850968\n",
      "Validation Recall: 0.9591439688715954\n",
      "Validation F1-Score: 0.7078248384781048\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Performance with 0.1 threshold\n",
      "\n",
      "Validation Accuracy: 0.539\n",
      "Validation Precision: 0.5275181723779855\n",
      "Validation Recall: 0.9883268482490273\n",
      "Validation F1-Score: 0.6878808395396073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define a function to evaluate the model with an adjustable threshold\n",
    "def evaluate_model_with_threshold(model, dataloader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_masks, labels = batch\n",
    "            input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_masks)\n",
    "            logits = outputs  # Directly use the output tensor\n",
    "            probabilities = torch.sigmoid(logits)[:, 1]  # Assuming the positive class is at index 1\n",
    "            predictions = (probabilities > threshold).long()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='binary')\n",
    "    recall = recall_score(all_labels, all_predictions, average='binary')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Evaluate on the validation/test set with a lower threshold to improve recall\n",
    "threshold = 0.7  # Adjust this value as needed\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n",
    "\n",
    "print()\n",
    "print('----------------------------------------------')\n",
    "print()\n",
    "\n",
    "threshold = 0.6  # Adjust this value as needed\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n",
    "\n",
    "print()\n",
    "print('----------------------------------------------')\n",
    "print()\n",
    "\n",
    "threshold = 0.5  # Adjust this value as needed\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n",
    "\n",
    "print()\n",
    "print('----------------------------------------------')\n",
    "print()\n",
    "\n",
    "threshold = 0.3  # Adjust this value as needed\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n",
    "\n",
    "print()\n",
    "print('----------------------------------------------')\n",
    "print()\n",
    "\n",
    "threshold = 0.2  # Adjust this value as needed\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n",
    "\n",
    "print()\n",
    "print('----------------------------------------------')\n",
    "print()\n",
    "\n",
    "threshold = 0.1  # Adjust this value as needed\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the performance we get after tuning our model and changing certains parameters (A/B testing manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings : batch size 32, epoch 10, learning rate 3e-5\n",
    "\n",
    "\n",
    "# Performance with 0.7 threshold\n",
    "\n",
    "# Validation Accuracy: 0.67\n",
    "# Validation Precision: 0.6348973607038123\n",
    "# Validation Recall: 0.8424124513618677\n",
    "# Validation F1-Score: 0.7240802675585284\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Performance with 0.6 threshold\n",
    "\n",
    "# Validation Accuracy: 0.662\n",
    "# Validation Precision: 0.6253561253561254\n",
    "# Validation Recall: 0.8540856031128404\n",
    "# Validation F1-Score: 0.7220394736842105\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Performance with 0.5 threshold\n",
    "\n",
    "# Validation Accuracy: 0.652\n",
    "# Validation Precision: 0.6130790190735694\n",
    "# Validation Recall: 0.8754863813229572\n",
    "# Validation F1-Score: 0.7211538461538461\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Performance with 0.3 threshold\n",
    "\n",
    "# Validation Accuracy: 0.619\n",
    "# Validation Precision: 0.5813953488372093\n",
    "# Validation Recall: 0.9241245136186771\n",
    "# Validation F1-Score: 0.7137490608564989\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Performance with 0.2 threshold\n",
    "\n",
    "# Validation Accuracy: 0.593\n",
    "# Validation Precision: 0.5608646188850968\n",
    "# Validation Recall: 0.9591439688715954\n",
    "# Validation F1-Score: 0.7078248384781048\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Performance with 0.1 threshold\n",
    "\n",
    "# Validation Accuracy: 0.539\n",
    "# Validation Precision: 0.5275181723779855\n",
    "# Validation Recall: 0.9883268482490273\n",
    "# Validation F1-Score: 0.6878808395396073"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics are only with a model trained on a small ammount of data and should be used to tune parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets now get performance metrics on the whole data set with a train_test split of 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same process as in the beginning but now with all the rows in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the text\n",
    "train_encodings = [preprocess_text_for_bert(text) for text in X_train]\n",
    "test_encodings = [preprocess_text_for_bert(text) for text in X_test]\n",
    "\n",
    "# Convert tokenized text data into numerical IDs for train and test sets\n",
    "train_input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in train_encodings]\n",
    "test_input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in test_encodings]\n",
    "\n",
    "# Pad sequences to a fixed length for train and test sets\n",
    "max_length = max(max(len(ids) for ids in train_input_ids), max(len(ids) for ids in test_input_ids))\n",
    "train_input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in train_input_ids]\n",
    "test_input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in test_input_ids]\n",
    "\n",
    "# Convert input_ids lists into PyTorch tensors for train and test sets\n",
    "train_input_ids = torch.tensor(train_input_ids)\n",
    "test_input_ids = torch.tensor(test_input_ids)\n",
    "\n",
    "# Create attention masks to indicate which tokens are actual words and which are padding tokens for train and test sets\n",
    "train_attention_masks = torch.where(train_input_ids != tokenizer.pad_token_id, torch.tensor(1), torch.tensor(0))\n",
    "test_attention_masks = torch.where(test_input_ids != tokenizer.pad_token_id, torch.tensor(1), torch.tensor(0))\n",
    "\n",
    "# Create labels tensors for train and test sets\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "test_labels = torch.tensor(y_test.values)\n",
    "\n",
    "# Create TensorDatasets for train and test sets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders for train and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the fine-tuning model\n",
    "num_labels = 2  # Number of output labels (machine-written, human-written)\n",
    "model = CustomBERTForClassification(num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Define number of training epochs\n",
    "num_epochs = 15\n",
    "\n",
    "# Move model and data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drago\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5760435861662815\n",
      "Epoch 2, Loss: 0.4528866478957628\n",
      "Epoch 3, Loss: 0.32058884799480436\n",
      "Epoch 4, Loss: 0.24398892097567257\n",
      "Epoch 5, Loss: 0.1766134725040511\n",
      "Epoch 6, Loss: 0.1380147894177782\n",
      "Epoch 7, Loss: 0.11504603306615824\n",
      "Epoch 8, Loss: 0.10283022353082504\n",
      "Epoch 9, Loss: 0.08404091031192557\n",
      "Epoch 10, Loss: 0.07039247675921376\n",
      "Epoch 11, Loss: 0.07504533402278628\n",
      "Epoch 12, Loss: 0.06518921642588746\n",
      "Epoch 13, Loss: 0.05957319792286542\n",
      "Epoch 14, Loss: 0.05700705631954694\n",
      "Epoch 15, Loss: 0.05563991794019545\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_masks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with 0.7 threshold\n",
      "\n",
      "Validation Accuracy: 0.6174496644295302\n",
      "Validation Precision: 0.6321285140562249\n",
      "Validation Recall: 0.5808118081180812\n",
      "Validation F1-Score: 0.6053846153846154\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Performance with 0.6 threshold\n",
      "\n",
      "Validation Accuracy: 0.6260253542132737\n",
      "Validation Precision: 0.6311475409836066\n",
      "Validation Recall: 0.6250922509225092\n",
      "Validation F1-Score: 0.6281053021876158\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Performance with 0.5 threshold\n",
      "\n",
      "Validation Accuracy: 0.6271439224459359\n",
      "Validation Precision: 0.6236933797909407\n",
      "Validation Recall: 0.6605166051660517\n",
      "Validation F1-Score: 0.6415770609318996\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Performance with 0.3 threshold\n",
      "\n",
      "Validation Accuracy: 0.6185682326621924\n",
      "Validation Precision: 0.6012195121951219\n",
      "Validation Recall: 0.7276752767527676\n",
      "Validation F1-Score: 0.6584307178631051\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Performance with 0.2 threshold\n",
      "\n",
      "Validation Accuracy: 0.6111111111111112\n",
      "Validation Precision: 0.589041095890411\n",
      "Validation Recall: 0.7616236162361624\n",
      "Validation F1-Score: 0.664306404892179\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Performance with 0.1 threshold\n",
      "\n",
      "Validation Accuracy: 0.5969425801640567\n",
      "Validation Precision: 0.5698267074413863\n",
      "Validation Recall: 0.8250922509225093\n",
      "Validation F1-Score: 0.6741031052155563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define a function to evaluate the model with an adjustable threshold\n",
    "def evaluate_model_with_threshold(model, dataloader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_masks, labels = batch\n",
    "            input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_masks)\n",
    "            logits = outputs  # Directly use the output tensor\n",
    "            probabilities = torch.sigmoid(logits)[:, 1]  # Assuming the positive class is at index 1\n",
    "            predictions = (probabilities > threshold).long()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='binary')\n",
    "    recall = recall_score(all_labels, all_predictions, average='binary')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Evaluate on the validation/test set with a lower threshold to improve recall\n",
    "threshold = 0.7  # Adjust this value as needed\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n",
    "\n",
    "print()\n",
    "print('----------------------------------------------')\n",
    "print()\n",
    "\n",
    "threshold = 0.6  # Adjust this value as needed\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n",
    "\n",
    "print()\n",
    "print('----------------------------------------------')\n",
    "print()\n",
    "\n",
    "threshold = 0.5  # Adjust this value as needed\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n",
    "\n",
    "print()\n",
    "print('----------------------------------------------')\n",
    "print()\n",
    "\n",
    "threshold = 0.3  # Adjust this value as needed\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n",
    "\n",
    "print()\n",
    "print('----------------------------------------------')\n",
    "print()\n",
    "\n",
    "threshold = 0.2  # Adjust this value as needed\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n",
    "\n",
    "print()\n",
    "print('----------------------------------------------')\n",
    "print()\n",
    "\n",
    "threshold = 0.1  # Adjust this value as needed\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_threshold(model, test_dataloader, threshold)\n",
    "print(f'Performance with {threshold} threshold')\n",
    "print()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1-Score: {val_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance with 0.7 threshold\n",
    "\n",
    "# Validation Accuracy: 0.6174496644295302\n",
    "# Validation Precision: 0.6321285140562249\n",
    "# Validation Recall: 0.5808118081180812\n",
    "# Validation F1-Score: 0.6053846153846154\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Performance with 0.6 threshold\n",
    "\n",
    "# Validation Accuracy: 0.6260253542132737\n",
    "# Validation Precision: 0.6311475409836066\n",
    "# Validation Recall: 0.6250922509225092\n",
    "# Validation F1-Score: 0.6281053021876158\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Performance with 0.5 threshold\n",
    "\n",
    "# Validation Accuracy: 0.6271439224459359\n",
    "# Validation Precision: 0.6236933797909407\n",
    "# Validation Recall: 0.6605166051660517\n",
    "# Validation F1-Score: 0.6415770609318996\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Performance with 0.3 threshold\n",
    "\n",
    "# Validation Accuracy: 0.6185682326621924\n",
    "# Validation Precision: 0.6012195121951219\n",
    "# Validation Recall: 0.7276752767527676\n",
    "# Validation F1-Score: 0.6584307178631051\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Performance with 0.2 threshold\n",
    "\n",
    "# Validation Accuracy: 0.6111111111111112\n",
    "# Validation Precision: 0.589041095890411\n",
    "# Validation Recall: 0.7616236162361624\n",
    "# Validation F1-Score: 0.664306404892179\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Performance with 0.1 threshold\n",
    "\n",
    "# Validation Accuracy: 0.5969425801640567\n",
    "# Validation Precision: 0.5698267074413863\n",
    "# Validation Recall: 0.8250922509225093\n",
    "# Validation F1-Score: 0.6741031052155563"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these metrics to predict the expected accuracy and recall of the model when performing with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets train the model with the entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the text\n",
    "encodings = [preprocess_text_for_bert(text) for text in X]  # Process the entire dataset\n",
    "\n",
    "# Convert tokenized text data into numerical IDs\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in encodings]\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_length = max(len(ids) for ids in input_ids)\n",
    "input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]\n",
    "\n",
    "# Convert input_ids list into PyTorch tensors\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# Create attention masks to indicate which tokens are actual words and which are padding tokens\n",
    "attention_masks = torch.where(input_ids != tokenizer.pad_token_id, torch.tensor(1), torch.tensor(0))\n",
    "\n",
    "# Create labels tensor\n",
    "labels = torch.tensor(y.values)  # Ensure y is the correct variable for labels\n",
    "\n",
    "# Create a TensorDataset for the entire dataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# The dataloader is now ready to be used for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the fine-tuning model\n",
    "num_labels = 2  # Number of output labels (machine-written, human-written)\n",
    "model = CustomBERTForClassification(num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Define number of training epochs\n",
    "num_epochs = 15\n",
    "\n",
    "# Move model and data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_masks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "#torch.save(model.state_dict(), \"MODEL_ALL_DATA_15_EPOCHS.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model with more epochs would probably improve it's performance, since a single training session takes 35 mins on my hardware, i decided to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets predict and tag new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And export it to an output.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS : Keep in mind that the prediction system below includes a threshold value, that way we can choose to maximise Accuracy or Recall\n",
    "\n",
    "A higher value around 0.5 to 0.6 will maximise Accuracy\n",
    "\n",
    "A lower value around 0.2 to 0.1 will maximise Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_for_predictions(text):\n",
    "    # Clean the text: join back into a single string without modifying the content\n",
    "    text = ' '.join(text.split())\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Add [CLS] and [SEP] tokens\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    # Convert tokens to numerical IDs\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drago\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to output.txt\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('MODEL_ALL_DATA_15_EPOCHS.pth', map_location=device))\n",
    "\n",
    "# Define the threshold we want\n",
    "    # Here we picked 0.1 to maximise Recall\n",
    "best_threshold = 0.1\n",
    "\n",
    "# Read the input file\n",
    "with open('REAL_DATA.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Preprocess the sentences\n",
    "input_ids = [preprocess_text_for_predictions(line.split('\\t', 1)[1].strip()) for line in lines]\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_length = max(len(ids) for ids in input_ids)\n",
    "input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.where(input_ids != tokenizer.pad_token_id, torch.tensor(1), torch.tensor(0))\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "dataset = TensorDataset(input_ids, attention_masks)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Function to make predictions with the specified threshold\n",
    "def predict_with_threshold(model, dataloader, threshold):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_masks = batch\n",
    "            input_ids, attention_masks = input_ids.to(device), attention_masks.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_masks)\n",
    "            probabilities = torch.sigmoid(outputs)[:, 1]  # Assuming the positive class is at index 1\n",
    "            predictions = (probabilities > threshold).long()\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "    return all_predictions\n",
    "\n",
    "# Get predictions for the sentences\n",
    "predictions = predict_with_threshold(model, dataloader, best_threshold)\n",
    "\n",
    "# Replace the number 2 with the predicted labels\n",
    "output_lines = []\n",
    "for line, prediction in zip(lines, predictions):\n",
    "    parts = line.strip().split('\\t', 1)\n",
    "    new_line = f\"{prediction}\\t{parts[1]}\\n\"\n",
    "    output_lines.append(new_line)\n",
    "\n",
    "# Save the modified content to a new file\n",
    "output_file_path = 'output.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.writelines(output_lines)\n",
    "\n",
    "print(f\"Predictions saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
